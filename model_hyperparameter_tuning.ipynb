{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rays-Uddin/used-car-price-prediction/blob/main/model_hyperparameter_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Importing required libraries and dataset"
      ],
      "metadata": {
        "id": "QAkNmRKRsvqh"
      },
      "id": "QAkNmRKRsvqh"
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns"
      ],
      "metadata": {
        "id": "i7Mg9M6wQRMx"
      },
      "execution_count": null,
      "outputs": [],
      "id": "i7Mg9M6wQRMx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Read dataset\n",
        "df = pd.read_csv('used_cars_preprocessed.csv', usecols=[\"brand\", \"model\", \"model_year\", \"engine_type\", \"fuel_type\", \"mileage in miles\", \"damage\", \"price_usd\"])\n",
        "df.sample(5)"
      ],
      "metadata": {
        "id": "YzJ6jXHqQAqH"
      },
      "execution_count": null,
      "outputs": [],
      "id": "YzJ6jXHqQAqH"
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset information\n",
        "df.info()"
      ],
      "metadata": {
        "id": "xXJ43vKXQsR_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "xXJ43vKXQsR_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32372d5a"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor, VotingRegressor, StackingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "import os\n",
        "import json"
      ],
      "id": "32372d5a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5174457d"
      },
      "outputs": [],
      "source": [
        "# Divide data into X (independent) and y (target) variables\n",
        "X = df.drop(columns=[\"price_usd\"])\n",
        "y = np.log(df[\"price_usd\"])"
      ],
      "id": "5174457d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "089f5016"
      },
      "outputs": [],
      "source": [
        "# Splitting the dataset to train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.21, random_state=123, stratify=X['model'])"
      ],
      "id": "089f5016"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbbcbe7e"
      },
      "outputs": [],
      "source": [
        "# Train and test data shape\n",
        "print(f\"X_train, X_test shape: {X_train.shape, X_test.shape}\")\n",
        "print(f\"y_train, y_test shape: {y_train.shape, y_test.shape}\")"
      ],
      "id": "dbbcbe7e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Defining custom class transformer"
      ],
      "metadata": {
        "id": "tXgPNK1ls_6z"
      },
      "id": "tXgPNK1ls_6z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39a55a47"
      },
      "outputs": [],
      "source": [
        "class OrderedTargetEncoder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A custom transformer for ordered target encoding of categorical features.\n",
        "\n",
        "    This encoder replaces categorical values with their smoothed target means,\n",
        "    and then maps these smoothed means to an ordered integer sequence.\n",
        "    It handles unknown categories by imputing with the global mean or raising an error.\n",
        "\n",
        "    Attributes:\n",
        "        columns (list): List of column names to encode. If None, all object-type columns are encoded.\n",
        "        smoothing (float): Smoothing factor for target mean calculation. Higher values reduce the impact of small categories.\n",
        "        handle_unknown (str): Strategy to handle unknown categories during transform.\n",
        "                              'mean': Imputes unknown categories with the mean order of known categories.\n",
        "                              'error': Raises a ValueError if unknown categories are encountered.\n",
        "        encodings_ (dict): Stores the smoothed target means for each category in each column.\n",
        "        global_mean_ (float): The overall mean of the target variable.\n",
        "        mapping_order_ (dict): Stores the ordered integer mapping for each category in each column.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, columns=None, smoothing=1.0, handle_unknown=\"mean\"):\n",
        "        \"\"\"\n",
        "        Initializes the OrderedTargetEncoder.\n",
        "\n",
        "        Args:\n",
        "            columns (list, optional): List of column names to encode. If None, all object-type columns are encoded.\n",
        "                                      Defaults to None.\n",
        "            smoothing (float, optional): Smoothing factor for target mean calculation. Defaults to 1.0.\n",
        "            handle_unknown (str, optional): Strategy to handle unknown categories ('mean' or 'error').\n",
        "                                            Defaults to \"mean\".\n",
        "        \"\"\"\n",
        "        self.columns = columns\n",
        "        self.smoothing = smoothing\n",
        "        self.handle_unknown = handle_unknown\n",
        "        self.encodings_ = {}\n",
        "        self.global_mean_ = None\n",
        "        self.mapping_order_ = {}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fits the encoder to the training data.\n",
        "\n",
        "        Calculates the global mean of the target and smoothed target means for each category\n",
        "        in the specified columns. It then creates an ordered integer mapping based on these smoothed means.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame or np.ndarray): The input features (training data).\n",
        "            y (pd.Series or np.ndarray): The target variable.\n",
        "\n",
        "        Returns:\n",
        "            self (OrderedTargetEncoder): The fitted encoder instance.\n",
        "        \"\"\"\n",
        "\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            X = pd.DataFrame(X, columns=self.columns or [f'col_{i}' for i in range(X.shape[1])])\n",
        "\n",
        "        y = pd.Series(y, name=\"target\")\n",
        "\n",
        "\n",
        "        if self.columns is None:\n",
        "            self.columns = X.columns\n",
        "\n",
        "        self.global_mean_ = y.mean()\n",
        "\n",
        "\n",
        "        for col in self.columns:\n",
        "            df_temp = pd.concat([X[col], y], axis=1)\n",
        "\n",
        "            stats = df_temp.groupby(col)[\"target\"].agg([\"mean\", \"count\"])\n",
        "\n",
        "            smoothing_factor = stats[\"count\"] / (stats[\"count\"] + self.smoothing)\n",
        "            stats[\"smoothed_mean\"] = smoothing_factor * stats[\"mean\"] + (1-smoothing_factor) * self.global_mean_\n",
        "\n",
        "            stats = stats.sort_values(by = \"smoothed_mean\")\n",
        "\n",
        "            ordered_map = {category: i for i, category in enumerate(stats.index)}\n",
        "\n",
        "            self.encodings_[col] = stats[\"smoothed_mean\"].to_dict()\n",
        "\n",
        "            self.mapping_order_[col] = ordered_map\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Transforms the input features using the fitted encoder.\n",
        "\n",
        "        Replaces categorical values with their corresponding ordered integer from the mapping.\n",
        "        Handles unknown categories based on the `handle_unknown` strategy.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame or np.ndarray): The input features to transform.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: The transformed features with ordered integer encoded columns.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If `handle_unknown` is 'error' and unknown categories are encountered.\n",
        "        \"\"\"\n",
        "\n",
        "        if not isinstance(X, pd.DataFrame):\n",
        "            X = pd.DataFrame(X, columns=self.columns)\n",
        "\n",
        "        X_encoded = X.copy()\n",
        "\n",
        "        for col in self.columns:\n",
        "\n",
        "            X_encoded[col] = X_encoded[col].map(self.mapping_order_[col])\n",
        "\n",
        "            if X_encoded[col].isnull().any() and self.handle_unknown == \"mean\":\n",
        "\n",
        "                mean_order = np.mean(list(self.mapping_order_[col].values()))\n",
        "                X_encoded[col] = X_encoded[col].fillna(mean_order)\n",
        "\n",
        "            elif X_encoded[col].isnull().any() and self.handle_unknown == \"error\":\n",
        "                raise ValueError(f\"Unknown categories encountered in column {col} and handle_unknown is set to 'error'.\")\n",
        "\n",
        "        return X_encoded"
      ],
      "id": "39a55a47"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "sQ6Ld5xNkVpT"
      },
      "id": "sQ6Ld5xNkVpT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Linear Regression"
      ],
      "metadata": {
        "id": "77a6vGx3VpND"
      },
      "id": "77a6vGx3VpND"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "913c9dd4"
      },
      "outputs": [],
      "source": [
        "# Pipeline for One-Hot Encoding categorical features\n",
        "ohe_pipe = Pipeline(steps = [\n",
        "    ('ohe', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "])\n",
        "\n",
        "# Pipeline for Ordered Target Encoding the 'model' feature, imputing unseen values, and scaling\n",
        "ote_pipe = Pipeline(steps = [\n",
        "    ('ote', OrderedTargetEncoder()),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Pipeline for imputing missing 'fuel_type' and then One-Hot Encoding\n",
        "fuel_pipe = Pipeline(steps = [\n",
        "    ('nan_fuel', SimpleImputer(strategy='constant', fill_value='Electric')),\n",
        "    ('ohe_fuel', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "])\n",
        "\n",
        "# Pipeline for imputing missing 'damage' and then One-Hot Encoding\n",
        "damage_pipe = Pipeline(steps = [\n",
        "    ('nan_damage', SimpleImputer(strategy='most_frequent')),\n",
        "    ('ohe_damage', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "])\n",
        "\n",
        "# Pipeline for imputing numerical features, applying polynomial features, and scaling\n",
        "poly_pipe = Pipeline(steps=[\n",
        "    (\"poly\", PolynomialFeatures()),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# ColumnTransformer to apply different preprocessing steps to different columns\n",
        "preprocessor_lr = ColumnTransformer(transformers = [\n",
        "    ('ohe_pipe', ohe_pipe, ['brand', 'engine_type']),\n",
        "    ('ote_pipe', ote_pipe, ['model']),\n",
        "    ('fuel_pipe', fuel_pipe, ['fuel_type']),\n",
        "    ('damage_pipe', damage_pipe, ['damage']),\n",
        "    ('poly_pipe', poly_pipe, ['model_year', 'mileage in miles']),\n",
        "], remainder = 'passthrough')"
      ],
      "id": "913c9dd4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup and run GridSearchCV for Linear Regression model\n",
        "pipe_lr = Pipeline([\n",
        "    ('preprocessor_lr', preprocessor_lr),\n",
        "    ('regressor_lr', LinearRegression())\n",
        "])\n",
        "\n",
        "param_grid_lr = {\n",
        "    'preprocessor_lr__ote_pipe__ote__smoothing': np.arange(0.1, 3, 0.3),\n",
        "    'preprocessor_lr__poly_pipe__poly__degree': [1, 2, 3],\n",
        "    'preprocessor_lr__poly_pipe__poly__include_bias': [False, True]\n",
        "}\n",
        "\n",
        "gs_lr = GridSearchCV(\n",
        "    estimator=pipe_lr,\n",
        "    param_grid=param_grid_lr,\n",
        "    cv=5,\n",
        "    scoring='r2',\n",
        "    n_jobs=-1\n",
        "    )\n",
        "\n",
        "gs_lr.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "lG0hY5nbg4sX"
      },
      "id": "lG0hY5nbg4sX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "138c5cd5"
      },
      "outputs": [],
      "source": [
        "# Display best parameters and evaluation metrics for Linear Regression\n",
        "best_parameters_lr = gs_lr.best_params_\n",
        "model_lr = gs_lr.best_estimator_\n",
        "r2_gs_lr = gs_lr.best_score_\n",
        "\n",
        "# Predictions\n",
        "y_pred_lr = model_lr.predict(X_test)\n",
        "\n",
        "# Residuals\n",
        "resid_lr = y_test - y_pred_lr\n",
        "\n",
        "# Performance metrics\n",
        "r_squared_lr = r2_score(y_test, y_pred_lr)\n",
        "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
        "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
        "\n",
        "print(f\"Best parameters from Grid Search CV: {best_parameters_lr}\")\n",
        "print(f\"Mean Absolute Error: {mae_lr:.3f}, Mean Squared Error: {mse_lr:.3f}, R squared grid search: {r2_gs_lr:.3f}, R squared prediction: {r_squared_lr:.3f}\")"
      ],
      "id": "138c5cd5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving best parameters of linear regression model to json\n",
        "params = {}\n",
        "\n",
        "params['Linear Regression'] = {\n",
        "    'best_value': r2_gs_lr,\n",
        "    'best_params': best_parameters_lr\n",
        "    }\n",
        "\n",
        "with open('best_params.json', 'w') as f:\n",
        "    json.dump(params, f, indent=4)"
      ],
      "metadata": {
        "id": "htCXMTuQPgOg"
      },
      "id": "htCXMTuQPgOg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "308f2fd9"
      },
      "outputs": [],
      "source": [
        "# Heteroscedasticity plot\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.scatter(y_pred_lr, resid_lr, alpha=0.5)\n",
        "plt.xlabel(\"y pred\")\n",
        "plt.ylabel(\"residuals\")\n",
        "plt.title(\"Heteroscedasticity Plot\")\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "id": "308f2fd9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot shows the residuals are spread randomly or uniformly around the zero horizontal red dotted line, this indicates homoscedasticity."
      ],
      "metadata": {
        "id": "_b6ZDgoTzIKf"
      },
      "id": "_b6ZDgoTzIKf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1c8453f"
      },
      "source": [
        "# 4. Hyperparameter tuning using optuna"
      ],
      "id": "a1c8453f"
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing the Optuna library for hyperparameter optimization.\n",
        "!pip install optuna"
      ],
      "metadata": {
        "id": "-sFe7eNJ4oWX"
      },
      "execution_count": null,
      "outputs": [],
      "id": "-sFe7eNJ4oWX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7246fd0a"
      },
      "outputs": [],
      "source": [
        "# Importing the Optuna library and its visualization tools.\n",
        "import optuna\n",
        "from optuna.visualization import plot_optimization_history, plot_slice, plot_param_importances"
      ],
      "id": "7246fd0a"
    },
    {
      "cell_type": "markdown",
      "id": "4c2f9c89",
      "metadata": {
        "id": "4c2f9c89"
      },
      "source": [
        "## 4.1. Regularized Linear Model\n",
        "### Ridge, Lasso and Elastic Net"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_regularize(trial):\n",
        "\n",
        "    '''\n",
        "    This function defines the objective for Optuna to optimize for regularized linear models.\n",
        "    It includes preprocessing steps and hyperparameter suggestions for Ridge, Lasso, and ElasticNet.\n",
        "    '''\n",
        "    # OrderedTarget Encoder parameters\n",
        "    smoothing = trial.suggest_float('smoothing', 0.0, 3.0)\n",
        "    ote = OrderedTargetEncoder(smoothing=smoothing)\n",
        "\n",
        "    # Polynomial parameters\n",
        "    degree = trial.suggest_int('degree', 1, 3)\n",
        "    include_bias = trial.suggest_categorical('include_bias', [False, True])\n",
        "    poly = PolynomialFeatures(\n",
        "        degree=degree, include_bias=include_bias\n",
        "    )\n",
        "\n",
        "    # Preprocessor pipelines and column tramsformer\n",
        "    ohe_pipe = Pipeline(steps = [\n",
        "        ('ohe', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    encoder_pipe = Pipeline(steps = [\n",
        "        ('ote', ote),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    engine_pipe = Pipeline([\n",
        "        ('impute_unknown', SimpleImputer(strategy='constant', fill_value='unknown/electric')),\n",
        "        ('ohe_engine', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    fuel_pipe = Pipeline(steps = [\n",
        "        ('nan_fuel', SimpleImputer(strategy='constant', fill_value='Electric')),\n",
        "        ('ohe_fuel', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    damage_pipe = Pipeline(steps = [\n",
        "        ('nan_damage', SimpleImputer(strategy='most_frequent')),\n",
        "        ('ohe_damage', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    poly_pipe = Pipeline(steps=[\n",
        "        (\"poly\", poly),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(transformers = [\n",
        "        ('ohe_pipe', ohe_pipe, ['brand']),\n",
        "        ('encoder_pipe', encoder_pipe, ['model']),\n",
        "        ('engine_pipe', engine_pipe, ['engine_type']),\n",
        "        ('fuel_pipe', fuel_pipe, ['fuel_type']),\n",
        "        ('damage_pipe', damage_pipe, ['damage']),\n",
        "        ('poly_pipe', poly_pipe, ['model_year', 'mileage in miles']),\n",
        "    ], remainder = 'passthrough')\n",
        "\n",
        "\n",
        "    # Regularization selection\n",
        "    regressor = trial.suggest_categorical('regressor', ['Ridge', 'Lasso', 'ElasticNet'])\n",
        "\n",
        "    # Ridge\n",
        "    if regressor == 'Ridge':\n",
        "\n",
        "      ridge_params = {\n",
        "        'alpha': trial.suggest_float('ridge_alpha', 1e-4, 10.0, log=True),\n",
        "        'fit_intercept': trial.suggest_categorical('ridge_fit_intercept', [False, True]),\n",
        "        'max_iter': trial.suggest_int('ridge_max_iter', 1000, 50000),\n",
        "        'tol': trial.suggest_float('ridge_tol', 1e-6, 1e-1, log=True),\n",
        "        'solver': trial.suggest_categorical('ridge_solver', ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'])\n",
        "      }\n",
        "\n",
        "      model = Ridge(**ridge_params, random_state=32)\n",
        "\n",
        "    # Lasso\n",
        "    elif regressor == 'Lasso':\n",
        "\n",
        "      lasso_params = {\n",
        "        'alpha': trial.suggest_float('lasso_alpha', 1e-4, 10.0, log=True),\n",
        "        'max_iter': trial.suggest_int('lasso_max_iter', 1000, 50000),\n",
        "        'tol': trial.suggest_float('lasso_tol', 1e-6, 1e-1, log=True),\n",
        "        'fit_intercept': trial.suggest_categorical('lasso_fit_intercept', [False, True]),\n",
        "        'selection': trial.suggest_categorical('lasso_selection', ['cyclic', 'random'])\n",
        "      }\n",
        "\n",
        "      model = Lasso(**lasso_params, random_state=35)\n",
        "\n",
        "    # ElasticNet\n",
        "    elif regressor == 'ElasticNet':\n",
        "\n",
        "      elasticnet_params = {\n",
        "        'alpha': trial.suggest_float('elasticnet_alpha', 1e-4, 10.0, log=True),\n",
        "        'max_iter': trial.suggest_int('elasticnet_max_iter', 1000, 50000),\n",
        "        'tol': trial.suggest_float('elasticnet_tol', 1e-6, 1e-1, log=True),\n",
        "        'l1_ratio': trial.suggest_float('elasticnet_l1_ratio', 0.2, 0.9),\n",
        "        'fit_intercept': trial.suggest_categorical('elasticnet_fit_intercept', [False, True])\n",
        "      }\n",
        "\n",
        "      model = ElasticNet(**elasticnet_params, random_state=37)\n",
        "\n",
        "\n",
        "    # Final preprocessor and model pipeline\n",
        "    pipe = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', model)\n",
        "    ])\n",
        "\n",
        "    # Cross validation\n",
        "    score = cross_val_score(pipe, X_train, y_train, cv=5, scoring='r2').mean()\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "9vOKYv-a_pkS"
      },
      "id": "9vOKYv-a_pkS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optuna optimization for regularized linear models.\n",
        "study_regularize = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=39))\n",
        "study_regularize.optimize(objective_regularize, n_trials=70, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "8BjNE8aSxmHX"
      },
      "id": "8BjNE8aSxmHX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7872452",
      "metadata": {
        "id": "e7872452"
      },
      "outputs": [],
      "source": [
        "# Best R2 score and parameters found by the Optuna study for regularized linear models.\n",
        "print(study_regularize.best_trial.value)\n",
        "study_regularize.best_trial.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e622e3f",
      "metadata": {
        "id": "3e622e3f"
      },
      "outputs": [],
      "source": [
        "# Display the optimization history plot from the Optuna study.\n",
        "plot_optimization_history(study_regularize).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6669816",
      "metadata": {
        "id": "e6669816"
      },
      "outputs": [],
      "source": [
        "# Display slice plots for the hyperparameters from the Optuna study.\n",
        "plot_slice(study_regularize).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e07831a",
      "metadata": {
        "id": "4e07831a"
      },
      "outputs": [],
      "source": [
        "# Display the hyperparameter importances plot from the Optuna study.\n",
        "plot_param_importances(study_regularize).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4781b91e",
      "metadata": {
        "id": "4781b91e"
      },
      "source": [
        "## 4.2. K Neighbor Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8eb17915",
      "metadata": {
        "id": "8eb17915"
      },
      "outputs": [],
      "source": [
        "def objective_knn(trial):\n",
        "\n",
        "    '''\n",
        "    This function defines the objective for Optuna to optimize for the K-Neighbors Regressor.\n",
        "    '''\n",
        "\n",
        "    # OTE parameters\n",
        "    smoothing = trial.suggest_float('smoothing', 0, 3)\n",
        "    ote = OrderedTargetEncoder(smoothing=smoothing)\n",
        "\n",
        "\n",
        "    # Polynomial parameters\n",
        "    degree = trial.suggest_int('degree', 1, 3)\n",
        "    include_bias = trial.suggest_categorical('include_bias', [False, True])\n",
        "    poly = PolynomialFeatures(\n",
        "        degree=degree, include_bias=include_bias\n",
        "    )\n",
        "\n",
        "    # Preprocessor pipelines and column tramsformer\n",
        "    ohe_pipe = Pipeline(steps = [\n",
        "        ('ohe', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    ote_pipe = Pipeline(steps = [\n",
        "        ('ote', ote),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    engine_pipe = Pipeline([\n",
        "        ('impute_unknown', SimpleImputer(strategy='constant', fill_value='unknown/electric')),\n",
        "        ('ohe_engine', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    fuel_pipe = Pipeline(steps = [\n",
        "        ('nan_fuel', SimpleImputer(strategy='constant', fill_value='Electric')),\n",
        "        ('ohe_fuel', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    damage_pipe = Pipeline(steps = [\n",
        "        ('nan_damage', SimpleImputer(strategy='most_frequent')),\n",
        "        ('ohe_damage', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    poly_pipe = Pipeline(steps=[\n",
        "        (\"poly\", poly),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(transformers = [\n",
        "        ('ohe_pipe', ohe_pipe, ['brand']),\n",
        "        ('ote_pipe', ote_pipe, ['model']),\n",
        "        ('engine_pipe', engine_pipe, ['engine_type']),\n",
        "        ('fuel_pipe', fuel_pipe, ['fuel_type']),\n",
        "        ('damage_pipe', damage_pipe, ['damage']),\n",
        "        ('poly_pipe', poly_pipe, ['model_year', 'mileage in miles']),\n",
        "    ], remainder = 'passthrough')\n",
        "\n",
        "\n",
        "    # K-neighbor regressor hyperparameters\n",
        "    params = {\n",
        "        'n_neighbors': trial.suggest_int('n_neighbors', 5, 30),\n",
        "        'weights': trial.suggest_categorical('weights', ['uniform', 'distance']),\n",
        "        'algorithm': trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute']),\n",
        "        'metric': trial.suggest_categorical('metric', ['euclidean', 'manhattan', 'minkowski']),\n",
        "        'p': trial.suggest_int('p', 1, 2)\n",
        "    }\n",
        "\n",
        "    model = KNeighborsRegressor(**params, n_jobs=-1)\n",
        "\n",
        "    # Final model and preprocessor pipeline\n",
        "    pipe = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', model)\n",
        "    ])\n",
        "\n",
        "    score = cross_val_score(pipe, X_train, y_train, cv=5, scoring='r2').mean()\n",
        "\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optuna optimization for K-Neighbors Regressor.\n",
        "study_knn = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
        "study_knn.optimize(objective_knn, n_trials=70, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "H_5CmSbvnSDt"
      },
      "id": "H_5CmSbvnSDt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd480b5b",
      "metadata": {
        "id": "bd480b5b"
      },
      "outputs": [],
      "source": [
        "# best R2 score and parameters found by the Optuna study for K-Neighbors Regressor.\n",
        "print(study_knn.best_trial.value)\n",
        "study_knn.best_trial.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8352856e",
      "metadata": {
        "id": "8352856e"
      },
      "outputs": [],
      "source": [
        "# display the optimization history plot from the Optuna study for K-Neighbors Regressor.\n",
        "plot_optimization_history(study_knn).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56311cf1",
      "metadata": {
        "id": "56311cf1"
      },
      "outputs": [],
      "source": [
        "# display slice plots for the K-Neighbors Regressor hyperparameters.\n",
        "plot_slice(study_knn).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9587aac0",
      "metadata": {
        "id": "9587aac0"
      },
      "outputs": [],
      "source": [
        "# display the hyperparameter importances plot for K-Neighbors Regressor.\n",
        "plot_param_importances(study_knn).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05082909",
      "metadata": {
        "id": "05082909"
      },
      "source": [
        "## 4.3 Support Vector Regreesion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea46650d",
      "metadata": {
        "id": "ea46650d"
      },
      "outputs": [],
      "source": [
        "def objective_svr(trial):\n",
        "\n",
        "    '''\n",
        "    This function defines the objective for Optuna to optimize for the Support Vector Regressor (SVR).\n",
        "    '''\n",
        "\n",
        "    # OTE parameters\n",
        "    smoothing = trial.suggest_float('smoothing', 0, 3)\n",
        "    ote = OrderedTargetEncoder(smoothing=smoothing)\n",
        "\n",
        "    # Preprocessor pipelines and column tramsformer\n",
        "    ohe_pipe = Pipeline(steps = [\n",
        "        ('ohe', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    ote_pipe = Pipeline(steps = [\n",
        "        ('ote', ote),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    engine_pipe = Pipeline([\n",
        "        ('impute_unknown', SimpleImputer(strategy='constant', fill_value='unknown/electric')),\n",
        "        ('ohe_engine', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    fuel_pipe = Pipeline(steps = [\n",
        "        ('nan_fuel', SimpleImputer(strategy='constant', fill_value='Electric')),\n",
        "        ('ohe_fuel', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    damage_pipe = Pipeline(steps = [\n",
        "        ('nan_damage', SimpleImputer(strategy='most_frequent')),\n",
        "        ('ohe_damage', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    scaler = Pipeline(steps=[\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "\n",
        "    preprocessor = ColumnTransformer(transformers = [\n",
        "        ('ohe_pipe', ohe_pipe, ['brand']),\n",
        "        ('ote_pipe', ote_pipe, ['model']),\n",
        "        ('engine_pipe', engine_pipe, ['engine_type']),\n",
        "        ('fuel_pipe', fuel_pipe, ['fuel_type']),\n",
        "        ('damage_pipe', damage_pipe, ['damage']),\n",
        "        ('scaler', scaler, ['model_year', 'mileage in miles'])\n",
        "    ], remainder = 'passthrough')\n",
        "\n",
        "\n",
        "    # SVR hyperparameters\n",
        "    param = {\n",
        "        'kernel': trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
        "        'degree': trial.suggest_int('degree', 1, 3),\n",
        "        'gamma': trial.suggest_float('gamma', 1e-3, 0.75, log = True),\n",
        "        'tol': trial.suggest_float('tol', 1e-4, 1, log = True),\n",
        "        'C': trial.suggest_float('C', 0.01, 50, log = True),\n",
        "        'epsilon': trial.suggest_float('epsilon', 1e-3, 1)\n",
        "    }\n",
        "\n",
        "    svr = SVR(**param)\n",
        "\n",
        "\n",
        "    # Final preprocessor and model pipeline\n",
        "    pipe = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', svr)\n",
        "    ])\n",
        "\n",
        "    ## Pruning\n",
        "    # cross validation\n",
        "    cv = KFold(n_splits=5, shuffle=True, random_state=111)\n",
        "    scores = []\n",
        "\n",
        "    # Manually iterate through folds for pruning\n",
        "    for i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
        "      #splitting data\n",
        "      X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "      y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "      # fit and get score\n",
        "      pipe.fit(X_fold_train, y_fold_train)\n",
        "      fold_score = pipe.score(X_fold_val, y_fold_val)\n",
        "      scores.append(fold_score)\n",
        "\n",
        "      # report for intermediate result to optuna\n",
        "      current_avg_score = np.mean(scores)\n",
        "      trial.report(current_avg_score, i)\n",
        "\n",
        "      # handle pruning to stop trial if it performs poorly\n",
        "      if trial.should_prune():\n",
        "          raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return np.mean(scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "590ee3cc",
      "metadata": {
        "id": "590ee3cc"
      },
      "outputs": [],
      "source": [
        "# Optuna optimization for the Support Vector Regressor (SVR).\n",
        "study_svr = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=43), pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=1, n_min_trials=2))\n",
        "study_svr.optimize(objective_svr, n_trials=70, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab6bc790",
      "metadata": {
        "id": "ab6bc790"
      },
      "outputs": [],
      "source": [
        "# best R2 score and parameters found by the Optuna study for SVR.\n",
        "print(study_svr.best_trial.value)\n",
        "study_svr.best_trial.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d01806a0",
      "metadata": {
        "id": "d01806a0"
      },
      "outputs": [],
      "source": [
        "# display the optimization history plot from the Optuna study for SVR.\n",
        "plot_optimization_history(study_svr).show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display slice plots for the SVR hyperparameters.\n",
        "plot_slice(study_svr).show()"
      ],
      "metadata": {
        "id": "s5flC-DaJ9La"
      },
      "id": "s5flC-DaJ9La",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48928ad4",
      "metadata": {
        "id": "48928ad4"
      },
      "outputs": [],
      "source": [
        "# display the hyperparameter importances plot for SVR.\n",
        "plot_param_importances(study_svr).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fe40a3d",
      "metadata": {
        "id": "6fe40a3d"
      },
      "source": [
        "## 4.4 Regression Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ca1f1c9",
      "metadata": {
        "id": "5ca1f1c9"
      },
      "source": [
        "### 4.4.1.Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82e96ec9",
      "metadata": {
        "id": "82e96ec9"
      },
      "outputs": [],
      "source": [
        "def objective_dt(trial):\n",
        "\n",
        "    '''\n",
        "    This function defines the objective for Optuna to optimize for the Decision Tree Regressor.\n",
        "    '''\n",
        "\n",
        "    # OTE parameters\n",
        "    smoothing = trial.suggest_float('smoothing', 0, 3)\n",
        "    ote = OrderedTargetEncoder(smoothing=smoothing)\n",
        "\n",
        "    # preprocessor pipelines and column transformer\n",
        "    ohe_pipe = Pipeline(steps = [\n",
        "        ('ohe', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    ote_pipe = Pipeline(steps = [\n",
        "        ('ote', ote)\n",
        "    ])\n",
        "\n",
        "    engine_pipe = Pipeline([\n",
        "        ('impute_unknown', SimpleImputer(strategy='constant', fill_value='unknown/electric')),\n",
        "        ('ohe_engine', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    fuel_pipe = Pipeline(steps = [\n",
        "        ('nan_fuel', SimpleImputer(strategy='constant', fill_value='Electric')),\n",
        "        ('ohe_fuel', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    damage_pipe = Pipeline(steps = [\n",
        "        ('nan_damage', SimpleImputer(strategy='most_frequent')),\n",
        "        ('ohe_damage', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "\n",
        "    preprocessor = ColumnTransformer(transformers = [\n",
        "        ('ohe_pipe', ohe_pipe, ['brand']),\n",
        "        ('ote_pipe', ote_pipe, ['model']),\n",
        "        ('engine_pipe', engine_pipe, ['engine_type']),\n",
        "        ('fuel_pipe', fuel_pipe, ['fuel_type']),\n",
        "        ('damage_pipe', damage_pipe, ['damage']),\n",
        "    ], remainder = 'passthrough')\n",
        "\n",
        "\n",
        "      # Decision tree hyperparameters\n",
        "    params = {\n",
        "        'criterion': trial.suggest_categorical('criterion', ['squared_error', 'friedman_mse', 'absolute_error', 'poisson']),\n",
        "        'max_depth': trial.suggest_categorical('max_depth', [8, 16, 32, 64, None]),\n",
        "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 100),\n",
        "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 100),\n",
        "        'max_features': trial.suggest_categorical('max_features', [0.33, 0.5, 0.75, None])\n",
        "    }\n",
        "\n",
        "    model = DecisionTreeRegressor(**params, random_state=45)\n",
        "\n",
        "    # Final preprocessor and model pipeline\n",
        "    pipe = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', model)\n",
        "    ])\n",
        "\n",
        "    # Pruning\n",
        "    # Cross validation\n",
        "    cv = KFold(n_splits=5, shuffle=True, random_state=138)\n",
        "    scores = []\n",
        "\n",
        "    # Manually iterate through folds for pruning\n",
        "    for i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
        "      #splitting data\n",
        "      X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "      y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "      # fit and get score\n",
        "      pipe.fit(X_fold_train, y_fold_train)\n",
        "      fold_score = pipe.score(X_fold_val, y_fold_val)\n",
        "      scores.append(fold_score)\n",
        "\n",
        "      # report for intermediate result to optuna\n",
        "      current_avg_score = np.mean(scores)\n",
        "      trial.report(current_avg_score, i)\n",
        "\n",
        "      # handle pruning to stop trial if it performs poorly\n",
        "      if trial.should_prune():\n",
        "          raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return np.mean(scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcc572ae",
      "metadata": {
        "id": "bcc572ae"
      },
      "outputs": [],
      "source": [
        "# Optuna optimization for the Decision Tree Regressor.\n",
        "study_dt = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=52), pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=1, n_min_trials=2))\n",
        "study_dt.optimize(objective_dt, n_trials=70, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35f8d04e",
      "metadata": {
        "id": "35f8d04e"
      },
      "outputs": [],
      "source": [
        "# best R2 score and parameters found by the Optuna study for Decision Tree Regressor.\n",
        "print(study_dt.best_trial.value)\n",
        "study_dt.best_trial.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fab19b9a",
      "metadata": {
        "id": "fab19b9a"
      },
      "outputs": [],
      "source": [
        "# display the optimization history plot from the Optuna study for Decision Tree Regressor.\n",
        "plot_optimization_history(study_dt).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33dc37e9",
      "metadata": {
        "id": "33dc37e9"
      },
      "outputs": [],
      "source": [
        "# display slice plots for the Decision Tree Regressor hyperparameters.\n",
        "plot_slice(study_dt).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02ff4f27",
      "metadata": {
        "id": "02ff4f27"
      },
      "outputs": [],
      "source": [
        "# display the hyperparameter importances plot for Decision Tree Regressor.\n",
        "plot_param_importances(study_dt).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f29aa2b6",
      "metadata": {
        "id": "f29aa2b6"
      },
      "source": [
        "### 4.4.2. Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68c10a32",
      "metadata": {
        "id": "68c10a32"
      },
      "outputs": [],
      "source": [
        "def objective_rf(trial):\n",
        "\n",
        "    '''\n",
        "    This function defines the objective for Optuna to optimize for the Random Forest Regressor.\n",
        "    '''\n",
        "\n",
        "    # OTE parameters\n",
        "    smoothing = trial.suggest_float('smoothing', 0, 3)\n",
        "    ote = OrderedTargetEncoder(smoothing=smoothing)\n",
        "\n",
        "    # preprocessor pipeline and column transformer\n",
        "    ohe_pipe = Pipeline(steps = [\n",
        "        ('ohe', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    ote_pipe = Pipeline(steps = [\n",
        "        ('ote', ote)\n",
        "    ])\n",
        "\n",
        "    engine_pipe = Pipeline([\n",
        "        ('impute_unknown', SimpleImputer(strategy='constant', fill_value='unknown/electric')),\n",
        "        ('ohe_engine', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    fuel_pipe = Pipeline(steps = [\n",
        "        ('nan_fuel', SimpleImputer(strategy='constant', fill_value='Electric')),\n",
        "        ('ohe_fuel', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    damage_pipe = Pipeline(steps = [\n",
        "        ('nan_damage', SimpleImputer(strategy='most_frequent')),\n",
        "        ('ohe_damage', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "\n",
        "    preprocessor = ColumnTransformer(transformers = [\n",
        "        ('ohe_pipe', ohe_pipe, ['brand']),\n",
        "        ('ote_pipe', ote_pipe, ['model']),\n",
        "        ('engine_pipe', engine_pipe, ['engine_type']),\n",
        "        ('fuel_pipe', fuel_pipe, ['fuel_type']),\n",
        "        ('damage_pipe', damage_pipe, ['damage']),\n",
        "    ], remainder = 'passthrough')\n",
        "\n",
        "\n",
        "    # Random forest hyperparameters\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
        "        'criterion': trial.suggest_categorical('criterion', ['squared_error', 'friedman_mse', 'absolute_error', 'poisson']),\n",
        "        'max_depth': trial.suggest_categorical('max_depth', [8, 16, 32, None]),\n",
        "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 50),\n",
        "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 50),\n",
        "        'max_features': trial.suggest_categorical('max_features', [0.33, 0.5, 0.75, None]),\n",
        "        'bootstrap': trial.suggest_categorical('bootstrap', [False, True])\n",
        "    }\n",
        "\n",
        "    model = RandomForestRegressor(**params, n_jobs=-1, random_state=13)\n",
        "\n",
        "\n",
        "    # Final pipeline\n",
        "    pipe = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', model)\n",
        "    ])\n",
        "\n",
        "    # Pruning\n",
        "    # Cross validation\n",
        "    cv = KFold(n_splits=5, shuffle=True, random_state=142)\n",
        "    scores = []\n",
        "\n",
        "    # Manually iterate through folds for pruning\n",
        "    for i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
        "      #splitting data\n",
        "      X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "      y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "      # fit and get score\n",
        "      pipe.fit(X_fold_train, y_fold_train)\n",
        "      fold_score = pipe.score(X_fold_val, y_fold_val)\n",
        "      scores.append(fold_score)\n",
        "\n",
        "      # report for intermediate result to optuna\n",
        "      current_avg_score = np.mean(scores)\n",
        "      trial.report(current_avg_score, i)\n",
        "\n",
        "      # handle pruning to stop trial if it performs poorly\n",
        "      if trial.should_prune():\n",
        "          raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return np.mean(scores)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optuna optimization for the Random Forest Regressor.\n",
        "study_rf = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=57), pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=1, n_min_trials=2))\n",
        "study_rf.optimize(objective_rf, n_trials=70, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "R49eDqXEr9Bd"
      },
      "id": "R49eDqXEr9Bd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe0f5331",
      "metadata": {
        "id": "fe0f5331"
      },
      "outputs": [],
      "source": [
        "# best R2 score and parameters found by the Optuna study for Random Forest Regressor.\n",
        "print(study_rf.best_trial.value)\n",
        "study_rf.best_trial.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "039abbbb",
      "metadata": {
        "id": "039abbbb"
      },
      "outputs": [],
      "source": [
        "# display the optimization history plot from the Optuna study for Random Forest Regressor.\n",
        "plot_optimization_history(study_rf).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae97fb54",
      "metadata": {
        "id": "ae97fb54"
      },
      "outputs": [],
      "source": [
        "# display slice plots for the Random Forest Regressor hyperparameters.\n",
        "plot_slice(study_rf).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3aea364c",
      "metadata": {
        "id": "3aea364c"
      },
      "outputs": [],
      "source": [
        "# displays the hyperparameter importances plot for Random Forest Regressor.\n",
        "plot_param_importances(study_rf).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a811149",
      "metadata": {
        "id": "5a811149"
      },
      "source": [
        "## 4.5. Boosting Ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "852fb3fd",
      "metadata": {
        "id": "852fb3fd"
      },
      "source": [
        "### 4.5.1. AdaBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b84cde3d",
      "metadata": {
        "id": "b84cde3d"
      },
      "outputs": [],
      "source": [
        "def objective_adaboost(trial):\n",
        "\n",
        "    '''\n",
        "    This function defines the objective for Optuna to optimize for the AdaBoost Regressor.\n",
        "    '''\n",
        "\n",
        "    # OTE parameters\n",
        "    smoothing = trial.suggest_float('smoothing', 0, 3)\n",
        "    ote = OrderedTargetEncoder(smoothing=smoothing)\n",
        "\n",
        "    # preprocessor pipeline and colimn transformer\n",
        "    ohe_pipe = Pipeline(steps = [\n",
        "        ('ohe', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    ote_pipe = Pipeline(steps = [\n",
        "        ('ote', ote)\n",
        "    ])\n",
        "\n",
        "    engine_pipe = Pipeline([\n",
        "        ('impute_unknown', SimpleImputer(strategy='constant', fill_value='unknown/electric')),\n",
        "        ('ohe_engine', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "        ])\n",
        "\n",
        "    fuel_pipe = Pipeline(steps = [\n",
        "        ('nan_fuel', SimpleImputer(strategy='constant', fill_value='Electric')),\n",
        "        ('ohe_fuel', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    damage_pipe = Pipeline(steps = [\n",
        "        ('nan_damage', SimpleImputer(strategy='most_frequent')),\n",
        "        ('ohe_damage', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "\n",
        "    preprocessor = ColumnTransformer(transformers = [\n",
        "        ('ohe_pipe', ohe_pipe, ['brand']),\n",
        "        ('ote_pipe', ote_pipe, ['model']),\n",
        "        ('engine_pipe', engine_pipe, ['engine_type']),\n",
        "        ('fuel_pipe', fuel_pipe, ['fuel_type']),\n",
        "        ('damage_pipe', damage_pipe, ['damage'])\n",
        "    ], remainder = 'passthrough')\n",
        "\n",
        "\n",
        "    # Adaptive boosting hyperparameters\n",
        "    tree_params = {\n",
        "    'max_depth': trial.suggest_int('max_depth', 1, 5),\n",
        "    'min_samples_split': trial.suggest_int('min_samples_split', 2, 50),\n",
        "    'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 50),\n",
        "    }\n",
        "    estimator = DecisionTreeRegressor(**tree_params, random_state=22)\n",
        "\n",
        "    params = {\n",
        "        'loss': trial.suggest_categorical('loss', ['linear', 'square', 'exponential']),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1, log = True)\n",
        "    }\n",
        "\n",
        "\n",
        "    model = AdaBoostRegressor(estimator = estimator, **params, random_state=63)\n",
        "\n",
        "    # Final pipeline\n",
        "    pipe= Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', model)\n",
        "    ])\n",
        "\n",
        "    # Pruning\n",
        "    # Cross validation\n",
        "    cv = KFold(n_splits=5, shuffle=True, random_state=153)\n",
        "    scores = []\n",
        "\n",
        "    # Manually iterate through folds for pruning\n",
        "    for i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
        "      #splitting data\n",
        "      X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "      y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "      # fit and get score\n",
        "      pipe.fit(X_fold_train, y_fold_train)\n",
        "      fold_score = pipe.score(X_fold_val, y_fold_val)\n",
        "      scores.append(fold_score)\n",
        "\n",
        "      # report for intermediate result to optuna\n",
        "      current_avg_score = np.mean(scores)\n",
        "      trial.report(current_avg_score, i)\n",
        "\n",
        "      # handle pruning to stop trial if it performs poorly\n",
        "      if trial.should_prune():\n",
        "          raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return np.mean(scores)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optuna optimization for the AdaBoost Regressor.\n",
        "study_adaboost = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=68), pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=1, n_min_trials=2))\n",
        "study_adaboost.optimize(objective_adaboost, n_trials=70, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "A1DC839Zfj5a"
      },
      "id": "A1DC839Zfj5a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb7b886f",
      "metadata": {
        "id": "eb7b886f"
      },
      "outputs": [],
      "source": [
        "# best R2 score and parameters found by the Optuna study for AdaBoost Regressor.\n",
        "print(study_adaboost.best_trial.value)\n",
        "study_adaboost.best_trial.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bde54be1",
      "metadata": {
        "id": "bde54be1"
      },
      "outputs": [],
      "source": [
        "# display the optimization history plot from the Optuna study for AdaBoost Regressor.\n",
        "plot_optimization_history(study_adaboost).show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display slice plots for the AdaBoost Regressor hyperparameters.\n",
        "plot_slice(study_adaboost).show()"
      ],
      "metadata": {
        "id": "hgiUrd17USv8"
      },
      "id": "hgiUrd17USv8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display the hyperparameter importances plot for AdaBoost Regressor.\n",
        "plot_param_importances(study_adaboost).show()"
      ],
      "metadata": {
        "id": "bV1hU1noUt_u"
      },
      "id": "bV1hU1noUt_u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e940a0d7",
      "metadata": {
        "id": "e940a0d7"
      },
      "source": [
        "### 4.5.2. GradientBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f5582a7",
      "metadata": {
        "id": "0f5582a7"
      },
      "outputs": [],
      "source": [
        "def objective_gradboost(trial):\n",
        "\n",
        "    '''\n",
        "    This function defines the objective for Optuna to optimize for the Gradient Boosting Regressor.\n",
        "    '''\n",
        "\n",
        "    # OTE parameters\n",
        "    smoothing = trial.suggest_float('smoothing', 0, 3)\n",
        "    ote = OrderedTargetEncoder(smoothing=smoothing)\n",
        "\n",
        "    # preprocessor pipelines and column transformer\n",
        "    ohe_pipe = Pipeline(steps = [\n",
        "        ('ohe', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    ote_pipe = Pipeline(steps = [\n",
        "        ('ote', ote)\n",
        "    ])\n",
        "\n",
        "    engine_pipe = Pipeline([\n",
        "        ('impute_unknown', SimpleImputer(strategy='constant', fill_value='unknown/electric')),\n",
        "        ('ohe_engine', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "        ])\n",
        "\n",
        "    fuel_pipe = Pipeline(steps = [\n",
        "        ('nan_fuel', SimpleImputer(strategy='constant', fill_value='Electric')),\n",
        "        ('ohe_fuel', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    damage_pipe = Pipeline(steps = [\n",
        "        ('nan_damage', SimpleImputer(strategy='most_frequent')),\n",
        "        ('ohe_damage', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "\n",
        "    preprocessor = ColumnTransformer(transformers = [\n",
        "        ('ohe_pipe', ohe_pipe, ['brand']),\n",
        "        ('ote_pipe', ote_pipe, ['model']),\n",
        "        ('engine_pipe', engine_pipe, ['engine_type']),\n",
        "        ('fuel_pipe', fuel_pipe, ['fuel_type']),\n",
        "        ('damage_pipe', damage_pipe, ['damage'])\n",
        "    ], remainder = 'passthrough')\n",
        "\n",
        "\n",
        "    # Gradient boosting hyperparameters\n",
        "    params = {\n",
        "        'criterion': trial.suggest_categorical('criterion', ['friedman_mse', 'squared_error']),\n",
        "        'loss': trial.suggest_categorical('loss', ['squared_error', 'absolute_error', 'huber', 'quantile']),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 700),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1, log=True),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 50),\n",
        "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 50),\n",
        "        'max_features': trial.suggest_categorical('max_features', [0.5, 0.75, None])\n",
        "    }\n",
        "\n",
        "    model = GradientBoostingRegressor(**params, random_state=72)\n",
        "\n",
        "\n",
        "    pipe= Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', model)\n",
        "    ])\n",
        "\n",
        "    # Pruning\n",
        "    # Cross validation\n",
        "    cv = KFold(n_splits=5, shuffle=True, random_state=159)\n",
        "    scores = []\n",
        "\n",
        "    # Manually iterate through folds for pruning\n",
        "    for i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
        "      #splitting data\n",
        "      X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "      y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "      # fit and get score\n",
        "      pipe.fit(X_fold_train, y_fold_train)\n",
        "      fold_score = pipe.score(X_fold_val, y_fold_val)\n",
        "      scores.append(fold_score)\n",
        "\n",
        "      # report for intermediate result to optuna\n",
        "      current_avg_score = np.mean(scores)\n",
        "      trial.report(current_avg_score, i)\n",
        "\n",
        "      # handle pruning to stop trial if it performs poorly\n",
        "      if trial.should_prune():\n",
        "          raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return np.mean(scores)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optuna optimization for the Gradient Boosting Regressor.\n",
        "study_gradboost = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=75), pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=1, n_min_trials=2))\n",
        "study_gradboost.optimize(objective_gradboost, n_trials=70, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "-mCMDI7WoDsc"
      },
      "id": "-mCMDI7WoDsc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ebe80dd",
      "metadata": {
        "id": "4ebe80dd"
      },
      "outputs": [],
      "source": [
        "# best R2 score and parameters found by the Optuna study for Gradient Boosting Regressor.\n",
        "print(study_gradboost.best_trial.value)\n",
        "print(study_gradboost.best_trial.params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64c1587f",
      "metadata": {
        "id": "64c1587f"
      },
      "outputs": [],
      "source": [
        "# display the optimization history plot from the Optuna study for Gradient Boosting Regressor.\n",
        "plot_optimization_history(study_gradboost).show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display slice plots for the Gradient Boosting Regressor hyperparameters.\n",
        "plot_slice(study_gradboost).show()"
      ],
      "metadata": {
        "id": "ev9CDoeyeysw"
      },
      "id": "ev9CDoeyeysw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display the hyperparameter importances plot for Gradient Boosting Regressor.\n",
        "plot_param_importances(study_gradboost).show()"
      ],
      "metadata": {
        "id": "cV_u8kldesKb"
      },
      "id": "cV_u8kldesKb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.6. XGB Regressor"
      ],
      "metadata": {
        "id": "ARlgonKAhJp3"
      },
      "id": "ARlgonKAhJp3"
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_xgb(trial):\n",
        "\n",
        "    '''\n",
        "    This function defines the objective for Optuna to optimize for the XGBoost Regressor.\n",
        "    '''\n",
        "\n",
        "    # OTE parameters\n",
        "    smoothing = trial.suggest_float('smoothing', 0, 3)\n",
        "    ote = OrderedTargetEncoder(smoothing=smoothing)\n",
        "\n",
        "    ohe_pipe = Pipeline(steps = [\n",
        "        ('ohe', OneHotEncoder(sparse_output = False))\n",
        "    ])\n",
        "\n",
        "    ote_pipe = Pipeline(steps = [\n",
        "        ('ote', ote)\n",
        "    ])\n",
        "\n",
        "    engine_pipe = Pipeline([\n",
        "        ('impute_unknown', SimpleImputer(strategy='constant', fill_value='unknown/electric')),\n",
        "        ('ohe_engine', OneHotEncoder(sparse_output = False))\n",
        "    ])\n",
        "\n",
        "    fuel_pipe = Pipeline(steps = [\n",
        "        ('nan_fuel', SimpleImputer(strategy='constant', fill_value='Electric')),\n",
        "        ('ohe_fuel', OneHotEncoder(sparse_output = False))\n",
        "    ])\n",
        "\n",
        "    damage_pipe = Pipeline(steps = [\n",
        "        ('nan_damage', SimpleImputer(strategy='most_frequent')),\n",
        "        ('ohe_damage', OneHotEncoder(sparse_output = False))\n",
        "    ])\n",
        "\n",
        "\n",
        "    preprocessor = ColumnTransformer(transformers = [\n",
        "        ('ohe_pipe', ohe_pipe, ['brand']),\n",
        "        ('ote_pipe', ote_pipe, ['model']),\n",
        "        ('engine_pipe', engine_pipe, ['engine_type']),\n",
        "        ('fuel_pipe', fuel_pipe, ['fuel_type']),\n",
        "        ('damage_pipe', damage_pipe, ['damage'])\n",
        "    ], remainder = 'passthrough')\n",
        "\n",
        "\n",
        "    # XGB Regressor parameters\n",
        "    params = {\n",
        "        'booster': trial.suggest_categorical('booster', ['gbtree']),\n",
        "        'eta': trial.suggest_float('eta', 1e-4, 1),\n",
        "        'gamma': trial.suggest_int('gamma', 0, 5),\n",
        "        'max_depth': trial.suggest_int('max_depth', 1, 12),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1, log=True),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-4, 1, log=True),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-4, 1, log=True)\n",
        "    }\n",
        "\n",
        "    model = XGBRegressor(**params, tree_method='hist', device='cuda')\n",
        "\n",
        "\n",
        "    # Final pipeline\n",
        "    pipe = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', model)\n",
        "    ])\n",
        "\n",
        "    # Pruning\n",
        "    # Cross validation\n",
        "    cv = KFold(n_splits=5, shuffle=True, random_state=165)\n",
        "    scores = []\n",
        "\n",
        "    # Manually iterate through folds for pruning\n",
        "    for i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
        "      #splitting data\n",
        "      X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "      y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "      # fit and get score\n",
        "      pipe.fit(X_fold_train, y_fold_train)\n",
        "      fold_score = pipe.score(X_fold_val, y_fold_val)\n",
        "      scores.append(fold_score)\n",
        "\n",
        "      # report for intermediate result to optuna\n",
        "      current_avg_score = np.mean(scores)\n",
        "      trial.report(current_avg_score, i)\n",
        "\n",
        "      # handle pruning to stop trial if it performs poorly\n",
        "      if trial.should_prune():\n",
        "          raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return np.mean(scores)"
      ],
      "metadata": {
        "id": "hs1OmtrOggZm"
      },
      "id": "hs1OmtrOggZm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optuna optimization for the XGBoost Regressor.\n",
        "study_xgb = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=108), pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=1, n_min_trials=2))\n",
        "study_xgb.optimize(objective_xgb, n_trials=70, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "WWu_41bL8ZdT"
      },
      "id": "WWu_41bL8ZdT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best R2 score and parameters found by the Optuna study for XGBoost Regressor.\n",
        "print(study_xgb.best_trial.value)\n",
        "study_xgb.best_trial.params"
      ],
      "metadata": {
        "id": "9Z2pAQ0cg6Sa"
      },
      "id": "9Z2pAQ0cg6Sa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display the optimization history plot from the Optuna study for XGBoost Regressor.\n",
        "plot_optimization_history(study_xgb).show()"
      ],
      "metadata": {
        "id": "O7QxGgQ1vy00"
      },
      "id": "O7QxGgQ1vy00",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display slice plots for the XGBoost Regressor hyperparameters.\n",
        "plot_slice(study_xgb).show()"
      ],
      "metadata": {
        "id": "BPwfB_IJv-ec"
      },
      "id": "BPwfB_IJv-ec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display the hyperparameter importances plot for XGBoost Regressor.\n",
        "plot_param_importances(study_xgb).show()"
      ],
      "metadata": {
        "id": "t7VajAwpwFmG"
      },
      "id": "t7VajAwpwFmG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.7. Ensemble"
      ],
      "metadata": {
        "id": "mrBh59zw54Ox"
      },
      "id": "mrBh59zw54Ox"
    },
    {
      "cell_type": "markdown",
      "id": "7f675c89",
      "metadata": {
        "id": "7f675c89"
      },
      "source": [
        "### 4.7.1. Voting Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a961fbec",
      "metadata": {
        "id": "a961fbec"
      },
      "outputs": [],
      "source": [
        "def objective_vote(trial):\n",
        "\n",
        "    '''\n",
        "    This function defines the objective for Optuna to optimize a Voting Regressor.\n",
        "    It combines SVR, Random Forest, and XGBoost with their respective preprocessors and hyperparameters.\n",
        "    '''\n",
        "\n",
        "    # Support Vector Regression preprocessor and model parameters\n",
        "\n",
        "    # OTE parameters\n",
        "    smoothing_svr = trial.suggest_float('smoothing', 0, 3)\n",
        "    ote_svr = OrderedTargetEncoder(smoothing=smoothing_svr)\n",
        "\n",
        "    ohe_pipe_svr = Pipeline(steps = [\n",
        "        ('ohe_svr', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    ote_pipe_svr = Pipeline(steps = [\n",
        "        ('ote', ote_svr),\n",
        "        ('scaler_svr', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    engine_pipe_svr = Pipeline([\n",
        "        ('impute_unknown_svr', SimpleImputer(strategy='constant', fill_value='unknown/electric')),\n",
        "        ('ohe_engine_svr', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    fuel_pipe_svr = Pipeline(steps = [\n",
        "        ('nan_fuel_svr', SimpleImputer(strategy='constant', fill_value='Electric')),\n",
        "        ('ohe_fuel_svr', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    damage_pipe_svr = Pipeline(steps = [\n",
        "        ('nan_damage_svr', SimpleImputer(strategy='most_frequent')),\n",
        "        ('ohe_damage_svr', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    scaler_svr = Pipeline(steps=[\n",
        "        ('scaler_svr', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    preprocessor_svr = ColumnTransformer(transformers = [\n",
        "        ('ohe_pipe_svr', ohe_pipe_svr, ['brand']),\n",
        "        ('ote_pipe_svr', ote_pipe_svr, ['model']),\n",
        "        ('engine_pipe_svr', engine_pipe_svr, ['engine_type']),\n",
        "        ('fuel_pipe_svr', fuel_pipe_svr, ['fuel_type']),\n",
        "        ('damage_pipe_svr', damage_pipe_svr, ['damage']),\n",
        "        ('scaler_svr', scaler_svr, ['model_year', 'mileage in miles'])\n",
        "    ], remainder = 'passthrough')\n",
        "\n",
        "    svr_params = {\n",
        "        'kernel': trial.suggest_categorical('kernel_svr', ['linear', 'poly', 'rbf']),\n",
        "        'degree': trial.suggest_int('degree_svr', 1, 3),\n",
        "        'gamma': trial.suggest_float('gamma_svr', 1e-3, 0.1, log=True),\n",
        "        'tol': trial.suggest_float('tol_svr', 1e-4, 1, log=True),\n",
        "        'C': trial.suggest_float('C_svr', 0.1, 20, log=True),\n",
        "        'epsilon': trial.suggest_float('epsilon_svr', 1e-3, 0.5)\n",
        "    }\n",
        "\n",
        "    svr = SVR(**svr_params)\n",
        "\n",
        "    # SVR pipeline\n",
        "    pipe_svr = Pipeline([\n",
        "        ('preprocessor_svr', preprocessor_svr),\n",
        "        ('regressor_svr', svr)\n",
        "    ])\n",
        "\n",
        "\n",
        "    ## Random Forest and XGB preprocessor and pipeline\n",
        "    # OTE parameters\n",
        "    smoothing = trial.suggest_float('smoothing', 0, 3)\n",
        "    ote = OrderedTargetEncoder(smoothing=smoothing)\n",
        "\n",
        "    ohe_pipe = Pipeline(steps = [\n",
        "        ('ohe', OneHotEncoder(sparse_output = False))\n",
        "    ])\n",
        "\n",
        "    ote_pipe = Pipeline(steps = [\n",
        "        ('ote', ote)\n",
        "    ])\n",
        "\n",
        "    engine_pipe = Pipeline([\n",
        "        ('impute_unknown', SimpleImputer(strategy='constant', fill_value='unknown/electric')),\n",
        "        ('ohe_engine', OneHotEncoder(sparse_output = False))\n",
        "    ])\n",
        "\n",
        "    fuel_pipe = Pipeline(steps = [\n",
        "        ('nan_fuel', SimpleImputer(strategy='constant', fill_value='Electric')),\n",
        "        ('ohe_fuel', OneHotEncoder(sparse_output = False))\n",
        "    ])\n",
        "\n",
        "    damage_pipe = Pipeline(steps = [\n",
        "        ('nan_damage', SimpleImputer(strategy='most_frequent')),\n",
        "        ('ohe_damage', OneHotEncoder(sparse_output = False))\n",
        "    ])\n",
        "\n",
        "\n",
        "    preprocessor = ColumnTransformer(transformers = [\n",
        "        ('ohe_pipe', ohe_pipe, ['brand']),\n",
        "        ('ote_pipe', ote_pipe, ['model']),\n",
        "        ('engine_pipe', engine_pipe, ['engine_type']),\n",
        "        ('fuel_pipe', fuel_pipe, ['fuel_type']),\n",
        "        ('damage_pipe', damage_pipe, ['damage']),\n",
        "    ], remainder = 'passthrough')\n",
        "\n",
        "    # Random Forest parameters\n",
        "    rf_params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators_rf', 100, 300),\n",
        "        'criterion': trial.suggest_categorical('dt_criterion_rf', ['friedman_mse', 'absolute_error', 'poisson']),\n",
        "        'max_depth': trial.suggest_categorical('dt_max_depth_rf', [8, 16, 32, None]),\n",
        "        'min_samples_split': trial.suggest_int('dt_min_samples_split_rf', 2, 25),\n",
        "        'min_samples_leaf': trial.suggest_int('dt_min_samples_leaf_rf', 1, 25),\n",
        "        'max_features': trial.suggest_categorical('dt_max_features_rf', [0.33, 0.5, 0.75, None]),\n",
        "        'bootstrap': trial.suggest_categorical('bootstrap_rf', [False, True])\n",
        "    }\n",
        "\n",
        "    rf = RandomForestRegressor(**rf_params, random_state=15)\n",
        "\n",
        "\n",
        "    # Random Forest Pipeline\n",
        "    pipe_rf = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor_rf', rf)\n",
        "    ])\n",
        "\n",
        "\n",
        "    # XGB Regressor parameters\n",
        "    xgb_params = {\n",
        "        'booster': trial.suggest_categorical('booster_xgb', ['gbtree']),\n",
        "        'eta': trial.suggest_float('eta_xgb', 1e-4, 0.1),\n",
        "        'gamma': trial.suggest_int('gamma_xgb', 0, 5),\n",
        "        'max_depth': trial.suggest_int('max_depth_xgb', 1, 12),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight_xgb', 1, 10),\n",
        "        'subsample': trial.suggest_float('subsample_xgb', 0.5, 1),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree_xgb', 0.5, 1),\n",
        "        'learning_rate': trial.suggest_float('learning_rate_xgb', 1e-4, 0.1, log=True),\n",
        "        'n_estimators': trial.suggest_int('n_estimators_xgb', 100, 1000),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha_xgb', 1e-4, 0.1, log=True),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda_xgb', 1e-4, 0.1, log=True)\n",
        "    }\n",
        "\n",
        "    xgb = XGBRegressor(**xgb_params, tree_method='hist', device='cuda')\n",
        "\n",
        "    # XGB regressor pipeline\n",
        "    pipe_xgb = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor_xgb', xgb)\n",
        "    ])\n",
        "\n",
        "\n",
        "    weight_svr = trial.suggest_float('weight_svr', 0.1, 2)\n",
        "    weight_rf = trial.suggest_float('weight_rf', 0.1, 2)\n",
        "    weight_xgb = trial.suggest_float('weight_xgb', 0.1, 2)\n",
        "\n",
        "    weights = [weight_svr, weight_rf, weight_xgb]\n",
        "\n",
        "    voting = VotingRegressor(\n",
        "        estimators=[('svr', pipe_svr),\n",
        "                    ('random_forest', pipe_rf),\n",
        "                    ('xgb', pipe_xgb)],\n",
        "        weights=weights,\n",
        "        n_jobs=-1\n",
        "        )\n",
        "\n",
        "    # Pruning\n",
        "    # Cross validation\n",
        "    cv = KFold(n_splits=5, shuffle=True, random_state=173)\n",
        "    scores = []\n",
        "\n",
        "    # Manually iterate through folds for pruning\n",
        "    for i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
        "      #splitting data\n",
        "      X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "      y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "      # fit and get score\n",
        "      voting.fit(X_fold_train, y_fold_train)\n",
        "\n",
        "      preds = voting.predict(X_fold_val)\n",
        "      fold_score = r2_score(y_fold_val, preds)\n",
        "      scores.append(fold_score)\n",
        "\n",
        "      # report for intermediate result to optuna\n",
        "      current_avg_score = np.mean(scores)\n",
        "      trial.report(current_avg_score, i)\n",
        "\n",
        "      # handle pruning to stop trial if it performs poorly\n",
        "      if trial.should_prune():\n",
        "          raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return np.mean(scores)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optuna optimization for the Voting Regressor.\n",
        "study_vote = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=83), pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=1, n_min_trials=2))\n",
        "study_vote.optimize(objective_vote, n_trials=70, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "QYZRJOCqEiH9"
      },
      "id": "QYZRJOCqEiH9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a074105",
      "metadata": {
        "id": "6a074105"
      },
      "outputs": [],
      "source": [
        "# best R2 score and parameters found by the Optuna study for Voting Regressor.\n",
        "print(study_vote.best_trial.value)\n",
        "study_vote.best_trial.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54d28a0e",
      "metadata": {
        "id": "54d28a0e"
      },
      "outputs": [],
      "source": [
        "# display the optimization history plot from the Optuna study for Voting Regressor.\n",
        "plot_optimization_history(study_vote).show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display slice plots for the Voting Regressor hyperparameters.\n",
        "plot_slice(study_vote).show()"
      ],
      "metadata": {
        "id": "uV4YxlUBnpjG"
      },
      "id": "uV4YxlUBnpjG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70ee54c8",
      "metadata": {
        "id": "70ee54c8"
      },
      "outputs": [],
      "source": [
        "# display the hyperparameter importances plot for Voting Regressor.\n",
        "plot_param_importances(study_vote).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bagging and Stacking Ensemble Warning\n",
        "\n",
        "**Note:** The following sections for Bagging and Stacking Ensemble optimization with Optuna (`study_bag` and `study_stack`) are included for demonstration and completeness of the hyperparameter tuning process. However, due to the extensive computational time required for these specific ensembles (estimated to be more than 3 hours each on the current hardware setup), their execution cells have been intentionally skipped to conserve resources and time during development. If you wish to run them, please be prepared for a prolonged execution duration."
      ],
      "metadata": {
        "id": "0AvSiwAUkuwS"
      },
      "id": "0AvSiwAUkuwS"
    },
    {
      "cell_type": "markdown",
      "id": "d4f56b76",
      "metadata": {
        "id": "d4f56b76"
      },
      "source": [
        "### 4.7.2. Bagging Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def objective_bag(trial):\n",
        "\n",
        "    # This function defines the objective for Optuna to optimize for a Bagging Regressor.\n",
        "    # It allows for different base estimators (Random Forest, SVR, and XGB) with their hyperparameters.\n",
        "\n",
        "    # OTE parameters\n",
        "    smoothing = trial.suggest_float('smoothing', 0, 3)\n",
        "    ote = OrderedTargetEncoder(smoothing=smoothing)\n",
        "\n",
        "    ohe_pipe = Pipeline(steps = [\n",
        "        ('ohe', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    ote_pipe = Pipeline(steps = [\n",
        "        ('ote', ote)\n",
        "    ])\n",
        "\n",
        "    engine_pipe = Pipeline([\n",
        "        ('impute_unknown', SimpleImputer(strategy='constant', fill_value='unknown/electric')),\n",
        "        ('ohe_engine', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    fuel_pipe = Pipeline(steps = [\n",
        "        ('nan_fuel', SimpleImputer(strategy='constant', fill_value='Electric')),\n",
        "        ('ohe_fuel', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    damage_pipe = Pipeline(steps = [\n",
        "        ('nan_damage', SimpleImputer(strategy='most_frequent')),\n",
        "        ('ohe_damage', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "\n",
        "    preprocessor = ColumnTransformer(transformers = [\n",
        "        ('ohe_pipe', ohe_pipe, ['brand']),\n",
        "        ('ote_pipe', ote_pipe, ['model']),\n",
        "        ('engine_pipe', engine_pipe, ['engine_type']),\n",
        "        ('fuel_pipe', fuel_pipe, ['fuel_type']),\n",
        "        ('damage_pipe', damage_pipe, ['damage']),\n",
        "    ], remainder = 'passthrough')\n",
        "\n",
        "\n",
        "    # Regressor selection\n",
        "    estimator = trial.suggest_categorical('regressor', ['RandomForest', 'SVR', 'XGB'])\n",
        "\n",
        "    # Random Forest\n",
        "    if estimator == 'RandomForest':\n",
        "      # Random Forest Regressor parameters\n",
        "      params_rf = {\n",
        "          'n_estimators': trial.suggest_int('n_estimators_rf', 100, 300),\n",
        "          'criterion': trial.suggest_categorical('criterion_rf', ['friedman_mse', 'absolute_error', 'poisson']),\n",
        "          'max_depth': trial.suggest_categorical('max_depth_rf', [8, 16, 32, None]),\n",
        "          'min_samples_split': trial.suggest_int('min_samples_split_rf', 2, 25),\n",
        "          'min_samples_leaf': trial.suggest_int('min_samples_leaf_rf', 1, 25),\n",
        "          'max_features': trial.suggest_categorical('max_features_rf', [0.5, 0.75, None]),\n",
        "          'bootstrap': trial.suggest_categorical('bootstrap_rf', [False, True]),\n",
        "      }\n",
        "\n",
        "      model = RandomForestRegressor(**params_rf, random_state=192)\n",
        "\n",
        "    # SVR\n",
        "    elif estimator == 'SVR':\n",
        "      # SVR parameters\n",
        "      params_svr = {\n",
        "          'kernel': trial.suggest_categorical('kernel_svr', ['linear', 'poly', 'rbf']),\n",
        "          'degree': trial.suggest_int('degree_svr', 1, 3),\n",
        "          'gamma': trial.suggest_float('gamma_svr', 1e-3, 0.75, log=True),\n",
        "          'tol': trial.suggest_float('tol_svr', 1e-4, 1, log=True),\n",
        "          'C': trial.suggest_float('C_svr', 0.01, 20, log=True),\n",
        "          'epsilon': trial.suggest_float('epsilon_svr', 1e-3, 1)\n",
        "      }\n",
        "\n",
        "      model = SVR(**params_svr)\n",
        "\n",
        "    # XGB\n",
        "    elif estimator == 'XGB':\n",
        "\n",
        "      # XGB Regressor parameters\n",
        "      params_xgb = {\n",
        "          'booster': trial.suggest_categorical('booster_xgb', ['gbtree']),\n",
        "          'eta': trial.suggest_float('eta_xgb', 1e-4, 0.1),\n",
        "          'gamma': trial.suggest_int('gamma_xgb', 0, 5),\n",
        "          'max_depth': trial.suggest_int('max_depth_xgb', 1, 12),\n",
        "          'min_child_weight': trial.suggest_int('min_child_weight_xgb', 1, 10),\n",
        "          'subsample': trial.suggest_float('subsample_xgb', 0.5, 1),\n",
        "          'colsample_bytree': trial.suggest_float('colsample_bytree_xgb', 0.5, 1),\n",
        "          'learning_rate': trial.suggest_float('learning_rate_xgb', 1e-4, 0.1, log=True),\n",
        "          'n_estimators': trial.suggest_int('n_estimators_xgb', 100, 1000),\n",
        "          'reg_alpha': trial.suggest_float('reg_alpha_xgb', 1e-4, 0.1, log=True),\n",
        "          'reg_lambda': trial.suggest_float('reg_lambda_xgb', 1e-4, 0.1, log=True)\n",
        "      }\n",
        "\n",
        "      model = XGBRegressor(**params_xgb, tree_method='hist', device='cuda')\n",
        "\n",
        "    # Bagging ensemble hyperparameters\n",
        "    params_bag = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators_bag', 50, 300),\n",
        "        'max_samples': trial.suggest_float('max_samples_bag', 0.5, 1),\n",
        "        'max_features': trial.suggest_float('max_features_bag', 0.5, 1),\n",
        "        'bootstrap': trial.suggest_categorical('bootstrap_bag', [False, True]),\n",
        "        'bootstrap_features': trial.suggest_categorical('bootstrap_features_bag', [False, True])\n",
        "    }\n",
        "\n",
        "    bagging = BaggingRegressor(**params_bag, estimator=model, n_jobs=-1, random_state=86)\n",
        "\n",
        "\n",
        "    # Final pipeline\n",
        "    pipe = Pipeline([\n",
        "        ('preprocessor_bagging', preprocessor),\n",
        "        ('regressor_bagging', bagging)\n",
        "    ])\n",
        "\n",
        "    # Pruning\n",
        "    cv = KFold(n_splits=5, shuffle=True, random_state=154)\n",
        "    scores = []\n",
        "\n",
        "    # Manually iterate through folds for pruning\n",
        "    for i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
        "      #splitting data\n",
        "      X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "      y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "      # fit and get score\n",
        "      pipe.fit(X_fold_train, y_fold_train)\n",
        "      fold_score = pipe.score(X_fold_val, y_fold_val)\n",
        "      scores.append(fold_score)\n",
        "\n",
        "      # report for intermediate result to optuna\n",
        "      current_avg_score = np.mean(scores)\n",
        "      trial.report(current_avg_score, i)\n",
        "\n",
        "      # handle pruning to stop trial if it performs poorly\n",
        "      if trial.should_prune():\n",
        "          raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return np.mean(scores)\n",
        "'''"
      ],
      "metadata": {
        "id": "PbxqbpzCcUdL"
      },
      "id": "PbxqbpzCcUdL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Optuna optimization for the Bagging Regressor.\n",
        "# study_bag = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=88), pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=1, n_min_trials=2))\n",
        "# study_bag.optimize(objective_bag, n_trials=30, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "eHANQOnzzH0_"
      },
      "id": "eHANQOnzzH0_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7465f3dd",
      "metadata": {
        "id": "7465f3dd"
      },
      "outputs": [],
      "source": [
        "# # best R2 score and parameters found by the Optuna study for Bagging Regressor.\n",
        "# print(study_bag.best_trial.value)\n",
        "# print(study_bag.best_trial.params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell displays the optimization history plot from the Optuna study for XGBoost Regressor.\n",
        "#plot_optimization_history(study_bag).show()"
      ],
      "metadata": {
        "id": "rrSE_I9dlTTv"
      },
      "execution_count": null,
      "outputs": [],
      "id": "rrSE_I9dlTTv"
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell displays slice plots for the XGBoost Regressor hyperparameters.\n",
        "#plot_slice(study_bag).show()"
      ],
      "metadata": {
        "id": "p-9fdQ2TlTTw"
      },
      "execution_count": null,
      "outputs": [],
      "id": "p-9fdQ2TlTTw"
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell displays the hyperparameter importances plot for XGBoost Regressor.\n",
        "#plot_param_importances(study_bag).show()"
      ],
      "metadata": {
        "id": "Kv-eUjOllTTw"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Kv-eUjOllTTw"
    },
    {
      "cell_type": "markdown",
      "id": "00adcfa1",
      "metadata": {
        "id": "00adcfa1"
      },
      "source": [
        "### 4.7.3. Stacking Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4457f6e7",
      "metadata": {
        "id": "4457f6e7"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "def objective_stack(trial):\n",
        "\n",
        "    # This function defines the objective for Optuna to optimize a Stacking Regressor.\n",
        "    # It combines Linear Regression, SVR, and Extra Trees as base estimators.\n",
        "\n",
        "    # OTE parameters\n",
        "    smoothing = trial.suggest_float('smoothing', 0, 3)\n",
        "    ote = OrderedTargetEncoder(smoothing=smoothing)\n",
        "\n",
        "    ohe_pipe = Pipeline(steps = [\n",
        "        ('ohe', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    ote_pipe = Pipeline(steps = [\n",
        "        ('ote', ote)\n",
        "    ])\n",
        "\n",
        "    engine_pipe = Pipeline([\n",
        "        ('impute_unknown', SimpleImputer(strategy='constant', fill_value='unknown/electric')),\n",
        "        ('ohe_engine', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    fuel_pipe = Pipeline(steps = [\n",
        "        ('nan_fuel', SimpleImputer(strategy='constant', fill_value='Electric')),\n",
        "        ('ohe_fuel', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "    damage_pipe = Pipeline(steps = [\n",
        "        ('nan_damage', SimpleImputer(strategy='most_frequent')),\n",
        "        ('ohe_damage', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "    ])\n",
        "\n",
        "\n",
        "    preprocessor = ColumnTransformer(transformers = [\n",
        "        ('ohe_pipe', ohe_pipe, ['brand']),\n",
        "        ('ote_pipe', ote_pipe, ['model']),\n",
        "        ('engine_pipe', engine_pipe, ['engine_type']),\n",
        "        ('fuel_pipe', fuel_pipe, ['fuel_type']),\n",
        "        ('damage_pipe', damage_pipe, ['damage']),\n",
        "    ], remainder = 'passthrough')\n",
        "\n",
        "\n",
        "    # Random Forest\n",
        "    params_rf = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators_rf', 100, 300),\n",
        "        'criterion': trial.suggest_categorical('criterion_rf', ['friedman_mse', 'absolute_error', 'poisson']),\n",
        "        'max_depth': trial.suggest_categorical('max_depth_rf', [8, 16, 32, None]),\n",
        "        'min_samples_split': trial.suggest_int('min_samples_split_rf', 2, 25),\n",
        "        'min_samples_leaf': trial.suggest_int('min_samples_leaf_rf', 1, 25),\n",
        "        'max_features': trial.suggest_categorical('max_features_rf', [0.5, 0.75, None]),\n",
        "        'bootstrap': trial.suggest_categorical('bootstrap_rf', [False, True]),\n",
        "    }\n",
        "\n",
        "    rf = RandomForestRegressor(**params_rf, random_state=199)\n",
        "\n",
        "    pipe_rf = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('RF', rf)\n",
        "    ])\n",
        "\n",
        "\n",
        "    # SVR\n",
        "    params_svr = {\n",
        "        'kernel': trial.suggest_categorical('kernel_svr', ['linear', 'poly', 'rbf']),\n",
        "        'degree': trial.suggest_int('degree_svr', 1, 3),\n",
        "        'gamma': trial.suggest_float('gamma_svr', 1e-3, 0.75, log=True),\n",
        "        'tol': trial.suggest_float('tol_svr', 1e-4, 1, log=True),\n",
        "        'C': trial.suggest_float('C_svr', 0.01, 50, log=True),\n",
        "        'epsilon': trial.suggest_float('epsilon_svr', 1e-3, 1)\n",
        "    }\n",
        "\n",
        "    svr = SVR(**params_svr)\n",
        "\n",
        "    pipe_svr = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('svr', svr)\n",
        "    ])\n",
        "\n",
        "\n",
        "    # XGB\n",
        "    params_xgb = {\n",
        "        'booster': trial.suggest_categorical('booster_xgb', ['gbtree']),\n",
        "        'eta': trial.suggest_float('eta_xgb', 1e-4, 0.1),\n",
        "        'gamma': trial.suggest_int('gamma_xgb', 0, 5),\n",
        "        'max_depth': trial.suggest_int('max_depth_xgb', 1, 12),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight_xgb', 1, 10),\n",
        "        'subsample': trial.suggest_float('subsample_xgb', 0.5, 1),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree_xgb', 0.5, 1),\n",
        "        'learning_rate': trial.suggest_float('learning_rate_xgb', 1e-4, 0.1, log=True),\n",
        "        'n_estimators': trial.suggest_int('n_estimators_xgb', 100, 1000),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha_xgb', 1e-4, 0.1, log=True),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda_xgb', 1e-4, 0.1, log=True)\n",
        "    }\n",
        "\n",
        "    xgb = XGBRegressor(**params_xgb, tree_method='hist', device='cuda')\n",
        "\n",
        "    pipe_xgb = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('xgb', xgb)\n",
        "    ])\n",
        "\n",
        "    estimators = [('RandomForest', pipe_rf), ('SVR', pipe_svr), ('XGB', pipe_xgb)]\n",
        "\n",
        "    final_estimator = LinearRegression()\n",
        "\n",
        "    stacking = StackingRegressor(\n",
        "        estimators=estimators,\n",
        "        final_estimator=final_estimator,\n",
        "        n_jobs=-1\n",
        "        )\n",
        "\n",
        "    cv = KFold(n_splits=5, shuffle=True, random_state=177)\n",
        "    scores = []\n",
        "\n",
        "    # Manually iterate through folds for pruning\n",
        "    for i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
        "      #splitting data\n",
        "      X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "      y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "      # fit and get score\n",
        "      stacking.fit(X_fold_train, y_fold_train)\n",
        "\n",
        "      preds = stacking.predict(X_fold_val)\n",
        "      fold_score = r2_score(y_fold_val, preds)\n",
        "      scores.append(fold_score)\n",
        "\n",
        "      # report for intermediate result to optuna\n",
        "      current_avg_score = np.mean(scores)\n",
        "      trial.report(current_avg_score, i)\n",
        "\n",
        "      # handle pruning to stop trial if it performs poorly\n",
        "      if trial.should_prune():\n",
        "          raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return np.mean(scores)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Optuna optimization for the Stacking Regressor.\n",
        "# study_stack = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=97), pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=1, n_min_trials=2))\n",
        "# study_stack.optimize(objective_stack, n_trials=30, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "AiGazgjHyhgO"
      },
      "id": "AiGazgjHyhgO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e442a1df",
      "metadata": {
        "id": "e442a1df"
      },
      "outputs": [],
      "source": [
        "# # This cell prints the best R2 score and parameters found by the Optuna study for Stacking Regressor.\n",
        "# print(study_stack.best_trial.value)\n",
        "# print(study_stack.best_trial.params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell displays the optimization history plot from the Optuna study for XGBoost Regressor.\n",
        "#plot_optimization_history(study_stack).show()"
      ],
      "metadata": {
        "id": "JyBFDlf0mYxn"
      },
      "execution_count": null,
      "outputs": [],
      "id": "JyBFDlf0mYxn"
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell displays slice plots for the XGBoost Regressor hyperparameters.\n",
        "#plot_slice(study_stack).show()"
      ],
      "metadata": {
        "id": "TPzQSB9xmYxo"
      },
      "execution_count": null,
      "outputs": [],
      "id": "TPzQSB9xmYxo"
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell displays the hyperparameter importances plot for XGBoost Regressor.\n",
        "#plot_param_importances(study_stack).show()"
      ],
      "metadata": {
        "id": "CNBibYKxmYxp"
      },
      "execution_count": null,
      "outputs": [],
      "id": "CNBibYKxmYxp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Save best parameters from every Optuna study to JSON"
      ],
      "metadata": {
        "id": "mBE5WC-NNVbU"
      },
      "id": "mBE5WC-NNVbU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77612ad9"
      },
      "source": [
        "def save_best_params_to_json(study, model_name, file_path):\n",
        "    \"\"\"\n",
        "    Saves the best parameters from an Optuna study to a JSON file.\n",
        "\n",
        "    Args:\n",
        "        study (optuna.study.Study): The Optuna study object.\n",
        "        model_name (str): The name of the model.\n",
        "        file_path (str): The path to the JSON file.\n",
        "    \"\"\"\n",
        "    best_value = study.best_trial.value\n",
        "    best_params = study.best_trial.params\n",
        "\n",
        "    all_params = {}\n",
        "\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, 'r') as f:\n",
        "            all_params = json.load(f)\n",
        "\n",
        "    all_params[model_name] = {\n",
        "        'best_value': best_value,\n",
        "        'best_params': best_params\n",
        "    }\n",
        "\n",
        "    with open(file_path, 'w') as f:\n",
        "        json.dump(all_params, f, indent=4)\n",
        "\n",
        "    print(f\"Best parameters for {model_name} saved to {file_path}\")"
      ],
      "id": "77612ad9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5be663d"
      },
      "source": [
        "save_best_params_to_json(study=study_regularize, model_name='Regularized_Linear_Models', file_path='best_params.json')"
      ],
      "id": "e5be663d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_best_params_to_json(study_knn, model_name='K-Neighbors_Regressor', file_path='best_params.json')"
      ],
      "metadata": {
        "id": "A5WX_CEcRICB"
      },
      "id": "A5WX_CEcRICB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_best_params_to_json(study=study_svr, model_name='Support_Vector_Regression', file_path='best_params.json')"
      ],
      "metadata": {
        "id": "_eLOSVMiRL0Y"
      },
      "id": "_eLOSVMiRL0Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_best_params_to_json(study=study_dt, model_name='Decision_Tree_Regressor', file_path='best_params.json')"
      ],
      "metadata": {
        "id": "y5kFIvA6RONV"
      },
      "id": "y5kFIvA6RONV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_best_params_to_json(study=study_rf, model_name='Random_Forest_Regressor', file_path='best_params.json')"
      ],
      "metadata": {
        "id": "m9j24knUTnW3"
      },
      "id": "m9j24knUTnW3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_best_params_to_json(study=study_adaboost, model_name='Ada_Boost_Regressor', file_path='best_params.json')"
      ],
      "metadata": {
        "id": "zOLKj8T2S1tr"
      },
      "id": "zOLKj8T2S1tr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_best_params_to_json(study=study_gradboost, model_name='Gradient_Boosting_Regressor', file_path='best_params.json')"
      ],
      "metadata": {
        "id": "4ZwKtAR4Tw0u"
      },
      "id": "4ZwKtAR4Tw0u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_best_params_to_json(study=study_xgb, model_name='XGB_Regressor', file_path='best_params.json')"
      ],
      "metadata": {
        "id": "AiSokpcom9f2"
      },
      "id": "AiSokpcom9f2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_best_params_to_json(study=study_vote, model_name='Voting_Ensumble', file_path='best_params.json')"
      ],
      "metadata": {
        "id": "NxroqT1idySW"
      },
      "id": "NxroqT1idySW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Final model training"
      ],
      "metadata": {
        "id": "rmm0lPkJzfln"
      },
      "id": "rmm0lPkJzfln"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best parameters from SVR optuna study\n",
        "best_param_svr = {\n",
        "    \"smoothing\": 0.4189992117346143,\n",
        "    \"kernel\": \"rbf\",\n",
        "    \"degree\": 3,\n",
        "    \"gamma\": 0.1096565189128103,\n",
        "    \"tol\": 0.0930844377161945,\n",
        "    \"C\": 3.620041916760818,\n",
        "    \"epsilon\": 0.14954686584240706\n",
        "}\n",
        "smoothing_svr = best_param_svr.pop('smoothing')\n",
        "best_param_svr"
      ],
      "metadata": {
        "id": "tUnBAjLL1tL6"
      },
      "id": "tUnBAjLL1tL6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ohe_pipe = Pipeline(steps = [\n",
        "    ('ohe', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "])\n",
        "\n",
        "ote_pipe = Pipeline(steps = [\n",
        "    ('ote', OrderedTargetEncoder(smoothing=smoothing_svr)),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "engine_pipe = Pipeline([\n",
        "    ('impute_unknown', SimpleImputer(strategy='constant', fill_value='unknown/electric')),\n",
        "    ('ohe_engine', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "])\n",
        "\n",
        "fuel_pipe = Pipeline(steps = [\n",
        "    ('nan_fuel', SimpleImputer(strategy='constant', fill_value='Electric')),\n",
        "    ('ohe_fuel', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "])\n",
        "\n",
        "damage_pipe = Pipeline(steps = [\n",
        "    ('nan_damage', SimpleImputer(strategy='most_frequent')),\n",
        "    ('ohe_damage', OneHotEncoder(sparse_output = False, drop = 'first'))\n",
        "])\n",
        "\n",
        "scaler = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers = [\n",
        "    ('ohe_pipe', ohe_pipe, ['brand']),\n",
        "    ('ote_pipe', ote_pipe, ['model']),\n",
        "    ('engine_pipe', engine_pipe, ['engine_type']),\n",
        "    ('fuel_pipe', fuel_pipe, ['fuel_type']),\n",
        "    ('damage_pipe', damage_pipe, ['damage']),\n",
        "    ('scaler', scaler, ['model_year', 'mileage in miles'])\n",
        "], remainder = 'passthrough')\n",
        "\n",
        "svr = SVR(**best_param_svr)\n",
        "\n",
        "# Final preprocessor and model pipeline\n",
        "pipe = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', svr)\n",
        "])\n",
        "\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = pipe.predict(X_test)\n",
        "\n",
        "# Residuals\n",
        "resid = y_test - y_pred\n",
        "\n",
        "# Performance metrics\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Absolute Error: {mae:.3f}, Mean Squared Error: {mse:.3f}, R squared prediction: {r_squared:.3f}\")"
      ],
      "metadata": {
        "id": "w0Zk0RSGO8xb"
      },
      "id": "w0Zk0RSGO8xb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save final trained svr pipeline to joblib\n",
        "import joblib\n",
        "joblib.dump(pipe, 'final_pipeline.joblib')"
      ],
      "metadata": {
        "id": "fo-wAF4k1gdm"
      },
      "id": "fo-wAF4k1gdm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yXmd8NL2DtbI"
      },
      "id": "yXmd8NL2DtbI",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}